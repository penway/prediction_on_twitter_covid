{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pandas as pd\n",
    "\n",
    "appName = \"Training\"\n",
    "spark = SparkSession.builder.appName(appName).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = spark.read.csv(\"../datasets/training_integrated_data.csv\", header=True, inferSchema=True)\n",
    "dataset.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dividedData = dataset.randomSplit([0.7, 0.3], 24) \n",
    "trainingData = dividedData[0]\n",
    "testingData = dividedData[1]  \n",
    "\n",
    "print (\"Training data rows:\", trainingData.count(), \"; Testing data rows:\", testingData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chose_column = \"valence\"\n",
    "\n",
    "feature_assembler = VectorAssembler(inputCols=[\"index\", \"cases\", \"newCases\", \"deaths\", \"positive\", \"negative\"], outputCol=\"features\")\n",
    "\n",
    "def Assemble(dataset, label: StringType):\n",
    "    featured = feature_assembler.transform(dataset)\n",
    "    return featured.select(\"features\", col(chose_column).alias(\"label\"))\n",
    "\n",
    "trainingDataFinal = Assemble(trainingData, chose_column)\n",
    "testingDataFinal = Assemble(testingData, chose_column)\n",
    "\n",
    "algorithm = GBTRegressor(\n",
    "    labelCol=\"label\", \n",
    "    featuresCol=\"features\", \n",
    "    maxIter=20\n",
    ")\n",
    "model = algorithm.fit(trainingDataFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict testing data using our model\n",
    "prediction = model.transform(testingDataFinal)\n",
    "#show some prediction results\n",
    "prediction.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import evaluator module for regression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "#define our evaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "#calculate RMSE of our trained model\n",
    "rmse = evaluator.evaluate(prediction)\n",
    "print (\"Root Mean Square Error (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dataset = spark.read.csv(\"../datasets/epidemic_pred.csv\", header=True, inferSchema=True).drop(\"date\")\n",
    "pred_dataset = pred_dataset.withColumn(\"index\", pred_dataset[\"index\"] + lit(580))\n",
    "pred_dataset.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = feature_assembler.transform(pred_dataset).select(\"features\")\n",
    "pred_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predicted_sentiment = np.float64(model.transform(pred_data).select(\"prediction\").collect())[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 600\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot_date(range(len(predicted_sentiment)), predicted_sentiment, marker='', linestyle='-')\n",
    "ax.set_aspect('auto')\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "95c28947ebbbbe2ea1e8c1b45930eb12374cab351e1c90100eecd3c0f8c27588"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('spark')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
